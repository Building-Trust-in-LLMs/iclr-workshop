<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Poster Session 1 - ICLR 2025 Workshop on Trustworthy LLM</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" href="assets/image.png" type="image/x-icon">
</head>
<body>
    <div class="header">
        <h1>ICLR 2025 Workshop<br> Building Trust in LLMs and LLM Applications: <br>From Guardrails to Explainability to Regulation</h1>
    </div>
    <div class="nav">
        <a href="index.html">HOME</a>
        <a href="speakers.html">SPEAKERS AND PANELISTS</a>
        <a href="cfp.html">CALL FOR PAPERS</a>
        <a href="schedule.html">WORKSHOP SCHEDULE</a>
    </div>
    <div class="content">
        <h2>Poster Session 1 (11:15am-12:15pm)</h2>
        <div class="poster-list">
            <div class="poster">
                <p>AdvBDGen: A Robust Framework for Generating Adaptive and Stealthy Backdoors in LLM Alignment Attacks</p>
            </div>
            <div class="poster">
                <p>Interpretable Steering of Large Language Models with Feature Guided Activation Additions</p>
            </div>
            <div class="poster">
                <p>The Differences Between Direct Alignment Algorithms are a Blur</p>
            </div>
            <div class="poster">
                <p>Towards Effective Discrimination Testing for Generative AI</p>
            </div>
            <div class="poster">
                <p>Scalable Fingerprinting of Large Language Models</p>
            </div>
            <div class="poster">
                <p>Prune 'n Predict: Optimizing LLM Decision-making with Conformal Prediction</p>
            </div>
            <div class="poster">
                <p>SPEX: Scaling Feature Interaction Explanations for LLMs</p>
            </div>
            <div class="poster">
                <p>Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis</p>
            </div>
            <div class="poster">
                <p>Differentially Private Retrieval Augmented Generation with Random Projection</p>
            </div>
            <div class="poster">
                <p>LLMS LOST IN TRANSLATION: M-ALERT UNCOVERS CROSS-LINGUISTIC SAFETY GAPS</p>
            </div>
            <div class="poster">
                <p>Unnatural Languages Are Not Bugs but Features for LLMs</p>
            </div>
            <div class="poster">
                <p>Working Memory Attack on LLMs</p>
            </div>
            <div class="poster">
                <p>No, Of Course I Can! Refusal Mechanisms Can Be Exploited Using Harmless Data</p>
            </div>
            <div class="poster">
                <p>VideoJail: Exploiting Video-Modality Vulnerabilities for Jailbreak Attacks on Multimodal Large Language Models</p>
            </div>
            <div class="poster">
                <p>Analyzing Memorization in Large Language Models through the Lens of Model Attribution</p>
            </div>
            <div class="poster">
                <p>PRUNING AS A DEFENSE: REDUCING MEMORIZATION IN LARGE LANGUAGE MODELS</p>
            </div>
            <div class="poster">
                <p>Antipodal Pairing and Mechanistic Signals in Dense SAE Latents</p>
            </div>
            <div class="poster">
                <p>Evaluating Text Humanlikeness via Self-Similarity Exponent</p>
            </div>
            <div class="poster">
                <p>Self-Ablating Transformers: More Interpretability, Less Sparsity</p>
            </div>
            <div class="poster">
                <p>Justified Trust in AI Fairness Assessment using Existing Metadata Entities</p>
            </div>
            <div class="poster">
                <p>Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings</p>
            </div>
            <div class="poster">
                <p>Fast Proxies for LLM Robustness Evaluation</p>
            </div>
            <div class="poster">
                <p>A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage</p>
            </div>
            <div class="poster">
                <p>Rethinking LLM Bias Probing Using Lessons from the Social Sciences</p>
            </div>
            <div class="poster">
                <p>AI Companions Are Not The Solution To Loneliness: Design Choices And Their Drawbacks</p>
            </div>
            <div class="poster">
                <p>A Generative Approach to LLM Harmfulness Detection with Red Flag Tokens</p>
            </div>
            <div class="poster">
                <p>A Missing Testbed for LLM Pre-Training Membership Inference Attacks</p>
            </div>
            <div class="poster">
                <p>Harmful Helper: Perform malicious tasks? Web AI agents might help</p>
            </div>
            <div class="poster">
                <p>Mechanistic Anomaly Detection for "Quirky'' Language Models</p>
            </div>
            <div class="poster">
                <p>Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates</p>
            </div>
            <div class="poster">
                <p>Latent Adversarial Training Improves the Representation of Refusal</p>
            </div>
            <div class="poster">
                <p>Reliable and Efficient Amortized Model-based Evaluation</p>
            </div>
            <div class="poster">
                <p>SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging</p>
            </div>
            <div class="poster">
                <p>Top of the CLASS: Benchmarking LLM Agents on Real-World Enterprise Tasks</p>
            </div>
            <div class="poster">
                <p>Evaluating and Mitigating the Safety Awareness-Execution Gaps of LM Agents</p>
            </div>
            <div class="poster">
                <p>Diagnostic Uncertainty: Teaching Language Models to Describe Open-Ended Uncertainty</p>
            </div>
            <div class="poster">
                <p>AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security</p>
            </div>
            <div class="poster">
                <p>Steering Fine-Tuning Generalization with Targeted Concept Ablation</p>
            </div>
            <div class="poster">
                <p>PATTERNS AND MECHANISMS OF CONTRASTIVE ACTIVATION ENGINEERING</p>
            </div>
            <div class="poster">
                <p>Hidden No More: Attack and Defending Private Third-Party LLM Inference</p>
            </div>
            <div class="poster">
                <p>MKA: Leveraging Cross-Lingual Consensus for Model Abstention</p>
            </div>
            <div class="poster">
                <p>Finding Sparse Autoencoder Representations Of Errors In CoT Prompting</p>
            </div>
            <div class="poster">
                <p>A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection</p>
            </div>
            <div class="poster">
                <p>Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering</p>
            </div>
            <div class="poster">
                <p>FiDeLiS: Faithful Reasoning in Large Language Models for Knowledge Graph Question Answering</p>
            </div>
            <div class="poster">
                <p>Mind the Gap: A Practical Attack on GGUF Quantization</p>
            </div>
            <div class="poster">
                <p>On-Premises LLM Deployment Demands a Middle Path: Preserving Privacy Without Sacrificing Model</p>
            </div>
            <div class="poster">
                <p>In-Context Meta Learning Induces Multi-Phase Circuit Emergence</p>
            </div>
            <div class="poster">
                <p>GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs</p>
            </div>
        </div>
    </div>
    <div class="footer">
        <p>ICLR 2025 Workshop - Building Trust in LLMs and LLM Applications: From Guardrails to Explainability to Regulation</p>
    </div>
</body>
</html> 