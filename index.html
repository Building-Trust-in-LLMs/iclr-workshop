<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ICLR 2025 Workshop on Trustworthy LLM</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" href="assets/icon.svg" type="image/x-icon">
</head>
<body>
<div class="header">
    <h1>ICLR 2025 Workshop<br> Building Trust in LLMs and LLM Applications: <br>From Guardrails to Explainability to Regulation</h1>
</div>
<div class="nav">
    <a href="index.html">HOME</a>
    <a href="getting-started.html">GETTING STARTED</a>
    <a href="tracks.html">TRACKS</a>
    <a href="prizes.html">PRIZES</a>
    <!--    <a href="leaderboards.html">LEADERBOARDS</a>-->
    <!--    <a href="workshop.html">WORKSHOP</a>-->
    <!--    <a href="faq.html">FAQ</a>-->
</div>
<div class="content">
    <h2>Welcome!</h2>
    <p>
        As the capabilities of Large Language Models (LLMs) continue to evolve, ensuring their reliability and trustworthiness becomes more critical than ever. Whether generating content or code, assisting in research, or being integrated into applications used by millions, the dependability of LLMs is paramount. This workshop brings together researchers, practitioners, and industry experts to explore cutting-edge advancements aimed at improving the trustworthiness of LLMs.
    </p>

    <h2>Workshop Objectives:</h2>
    <p>Our goal is to address the key challenges that arise in deploying LLMs in real-world environments by focusing on the following key topics:</p>

    <h3>1. Error Detection and Correction during Content/Code Generation</h3>
    <p>How can we identify and rectify errors in real-time during the generation process to improve the accuracy and utility of LLMs? This session explores error detection systems and strategies for content/code refinement.</p>

    <h3>2. Improving Reliability and Truthfulness of LMs</h3>
    <p>From mitigating hallucinations to correcting miscalibrated responses, we dive deep into techniques aimed at reducing distribution shifts and boosting the factual accuracy and consistency of LLM outputs.</p>

    <h3>3. Explainability and Interpretability of LM Responses</h3>
    <p>Understanding how LLMs arrive at their conclusions is essential for fostering trust. We’ll discuss methods to make LMs more transparent and interpretable, ensuring users can assess their reliability.</p>

    <h3>4. Robustness of LMs</h3>
    <p>Addressing prompt attacks and the interventional effects of data and input manipulation is crucial. We explore how to build models that remain resilient and robust even in adversarial or out-of-distribution settings.</p>

    <h3>5. Unlearning Techniques for LMs</h3>
    <p>In today’s dynamic world, the ability of LLMs to “unlearn” outdated or incorrect information is critical. This session focuses on strategies that allow models to forget specific data while retaining valuable knowledge.</p>

    <h3>6. Fairness of LMs</h3>
    <p>Biases, whether social, stereotype, or preference-based, pose significant ethical challenges in LLM deployment. Our discussions will center on techniques to reduce these biases and ensure fair and equitable outputs.</p>

    <h3>7. Guardrails and Regulations for LMs</h3>
    <p>Ensuring that LLMs are emotionally aware, detect toxic language, and avoid offensive or insensitive content is vital in creating responsible AI systems. We'll also explore emerging frameworks for regulating AI behavior.</p>

    <h3>8. Metrics, Benchmarks, and Evaluation of Trustworthy LMs</h3>
    <p>To track progress and set standards, we need reliable metrics and evaluation benchmarks. This session will focus on establishing industry standards for assessing the trustworthiness and reliability of LLMs.</p>

    <h2>Join us in shaping the future of trustworthy AI!</h2>
    <p>
        Whether you are an academic, industry researcher, or practitioner, this workshop is an opportunity to exchange ideas, share insights, and collaborate on creating LLMs that are not only powerful but also responsible. Let's work together to build models that the world can trust.
    </p>


    <!--    <h2>Organizers</h2>-->

    <h2>Contact</h2>
    <ul>
        <li> To contact the organizers, please email <a
                href="mailto:erasinginvisible@googlegroups.com">erasinginvisible@googlegroups.com</a>
        </li>
        <li> Please join our <a
                href="https://join.slack.com/t/erasingtheinvisible/shared_invite/zt-2m8abcw3m-SmG~focrUH6npGxxBxh03g">Slack</a>
            to receive the latest announcements, engage in discussions, find teammates, and ask
            questions.
        </li>
    </ul>
    <h2>Sponsors</h2>
    <div class="sponsors-logos">
        <a href="https://www.umiacs.umd.edu/" target="_blank">
            <img src="assets/umiacs-building-logo.png" alt="UMIACS Logo" title="UMIACS">
        </a>
        <!--        <a href="http://sponsor2.com" target="_blank">-->
        <!--            <img src="images/sponsors/sponsor2.png" alt="Sponsor 2 Logo" title="Sponsor 2">-->
        <!--        </a>-->
        <!-- Add more sponsors as needed -->
    </div>
    <h2>Organizers</h2>
    (* indicates Lead Organizers)
    <div class="photo-grid">
        <figure class="photo">
            <img src="assets/people/rnarayanam.png" alt="Ramasuri Narayanam">
            <figcaption alt="Ramasuri Narayanam"><a href="https://research.adobe.com/person/ramasuri-narayanam/">Ramasuri Narayanam<sup>*</sup></a><br>Adobe </figcaption>
        </figure>
        <figure class="photo">
            <img src="assets/people/micah.jpg" alt="Micah Goldblum">
            <figcaption alt="Micah Goldblum"><a href="https://goldblum.github.io/">Micah Goldblum<sup>*</sup></a><br>Columbia</figcaption>
        </figure>
        <figure class="photo">
            <img src="assets/people/bang.jpg" alt="Bang An">
            <figcaption alt="Bang An"><a href="https://bangann.github.io/">Bang An<sup>*</sup></a><br>UMD </figcaption>
        </figure>
        <figure class="photo">
            <img src="assets/people/soumya.jpg" alt="Soumyabrata Pal">
            <figcaption alt="Soumyabrata Pal"><a href="https://soumyabratap.github.io/">Soumyabrata Pal<sup>*</sup></a><br>Adobe</figcaption>
        </figure>
        <figure class="photo">
            <img src="assets/people/shsaini.jpeg" alt="Shiv Saini">
            <figcaption alt="Shiv Saini"><a href="https://research.adobe.com/person/shiv-kumar-saini/">Shiv Saini</a><br>Adobe </figcaption>
        </figure>
        <!-- Add more photos as needed -->
    </div>
    <pre><code>@inproceedings{
an2024waves,
title={{WAVES}: Benchmarking the Robustness of Image Watermarks},
author={Bang An and Mucong Ding and Tahseen Rabbani and Aakriti Agrawal and Yuancheng Xu and Chenghao Deng and Sicheng Zhu and Abdirisak Mohamed and Yuxin Wen and Tom Goldstein and Furong Huang},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=URtUYfC3GA}
}</code></pre>

<h1>Acknowledgements</h1>

<p>We would like to extend our heartfelt thanks to all those who made this Workshop on Trustworthy LLMs a success. This event would not have been possible without valuable contributions from several individuals.
Lastly, we express our gratitude to everyone working towards creating more trustworthy, reliable, and ethical AI systems. It is only through collaboration and shared knowledge that we can continue to push the boundaries of what is possible in AI while ensuring it remains safe and responsible for all.
Thank you all for being part of this journey!</p>




</div>
<div class="footer">
    <p>ICLR 2025 Workshop - Erasing the Invisible: A Stress-Test Challenge for Image
        Watermarks</p>
</div>
</body>
</html>
