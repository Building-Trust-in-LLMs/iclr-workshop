<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ICLR 2025 Workshop on Trustworthy LLM</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" href="assets/image.png" type="image/x-icon">
</head>
<body>
<div class="header">
    <h1>ICLR 2025 Workshop<br> Building Trust in LLMs and LLM Applications: <br>From Guardrails to Explainability to Regulation</h1>
</div>
<div class="nav">
    <a href="index.html">HOME</a>
    <a href="speakers.html">SPEAKERS AND PANELISTS</a>
    <a href="cfp.html">CALL FOR PAPERS</a>
    <a href="schedule.html">WORKSHOP SCHEDULE</a>
    <a href="format.html">WORKSHOP FORMAT</a>
    <!--    <a href="leaderboards.html">LEADERBOARDS</a>-->
    <!--    <a href="workshop.html">WORKSHOP</a>-->
    <!--    <a href="faq.html">FAQ</a>-->
</div>
<div class="content">
    <h2>Welcome!</h2>
    <p>
        As Large Language Models (LLMs) are rapidly adopted across diverse industries, concerns around their trustworthiness, safety, and ethical implications increasingly motivate academic research, industrial development, and legal innovation. LLMs are increasingly integrated into complex applications, where they must navigate challenges related to data privacy, regulatory compliance, and dynamic user interactions.  These complex applications amplify the potential of LLMs to violate the trust of humans.  Ensuring the trustworthiness of LLMs is paramount as they transition from standalone tools to integral components of real-world applications used by millions.

This workshop addresses the unique challenges posed by the deployment of LLMs, ranging from guardrails to explainability to regulation and beyond. The proposed workshop will bring together researchers and practitioners from academia and industry to explore cutting-edge solutions for improving the trustworthiness of LLMs and LLM-driven applications.  The workshop will feature invited talks, a panel discussion, interactive breakout discussion sessions, and poster presentations, fostering rich dialogue and knowledge exchange. We aim to bridge the gap between foundational research and the practical challenges of deploying LLMs in trustworthy, use-centric systems.

    </p>
    
    <!-- <h2>Motivation</h2>
    <p>
        Large language models (LLMs) are extensively and increasingly used in complex and transformative applications across industries such as healthcare, finance, legal advisory, education, and entertainment.
The growing reliance on these models has surfaced significant challenges related to trustworthiness, safety, and ethical use, making the theme of building trust in language models both timely as well as vital. In practice, LLMs are often part of large pipelines with many moving parts, perhaps including retrieval, user interactions, code execution, API-use, or guardrails.  Integrating LLMs into complex pipelines poses numerous additional challenges to trustworthiness.  For example, retrieval capabilities may enable models to violate copyright laws or user interactions may expose models to new and unexpected data. 
As LLMs go from standalone use to full-fledged applications, ensuring trustworthiness is an urgent priority. 

    </p>
    
    <h2>Focus</h2>
    <p>
        While previous workshops have addressed an extensive list of LLM properties, including theory, security and privacy, as well as technical problems such as model optimization and fairness, there is a notable gap when it comes to the trustworthiness of LLM applications such as AI assistants and co-pilots. Building such AI systems has become a widespread topic of academic and industrial research and also a central business strategy for organizations across numerous domains. Meanwhile, there has been little effort to bring together researchers and practitioners working on the unique scientific challenges and systems aspects of developing such LLM-driven systems. 
    </p>

    <h2>Review process:</h2>
    <p>
        Our review process for submitted materials will be double-blind (conducted via OpenReview) to mitigate institutional and author biases. The program will be curated to ensure a wide representation of research areas while upholding the standards of quality set by the double-blind review process. Consequently, our workshop will benefit from a diverse cohort of participants, and we will invite several contributors to speak alongside our primary invitees.

We will implement a reviewing mentorship program, inspired by several previous workshops, designed to foster the growth and development of junior reviewers. Within this initiative, junior reviewers will be paired with senior reviewers. These mentor-mentee relationships aim to ensure that junior reviewers receive real-time feedback, guidance, and mentorship as they navigate the complexities of crafting insightful and constructive reviews for workshop submissions. By facilitating this collaborative and educational process, we hope not only to elevate the quality of reviews but also to cultivate the next generation of expert reviewers in the field.

    </p>

    <p>This practical orientation sets our proposed workshop apart by focusing on:</p>
<ul style="list-style-type: disc; margin-left: 7.5mm; padding-left: 0; list-style-position: inside;">
    <li><strong>Deployed systems.</strong> We focus on research that addresses the types of challenges faced when deploying LLMs in real-world systems, for example ones used by millions of users. Such challenges include ensuring reliability and scalability, as well as maintaining performance under diverse and sometimes unpredictable conditions.</li>
    <li><strong>User interactions.</strong> The way users interact with LLM applications significantly impacts trust. We explore research on feedback loops and interaction models that enhance user trust and satisfaction.</li>
    <li><strong>Context-specific challenges.</strong> Often applications must navigate industry specific regulations and ensure compliance with laws and standards such as HIPAA in healthcare, GDPR in data privacy, and financial regulations.</li>
</ul>
    <p> While research on the above topics may have historically been limited to industry, a large academic community now develops full-fledged AI agents equipped with retrieval or tool-use capabilities, studies human-LLM interactions, or proposes legal or regulatory frameworks for ensuring safety.  Therefore, a workshop on LLM trustworthiness that extends beyond the models themselves is timely and will engage a large audience at ICLR.  Moreover, we encourage research not just on full-fledged systems but also on individual LLM behaviors, for example bias or unreliability, which may surface in applications.</p> -->
    
    <h2>Workshop Scope:</h2>
    <p>This workshop has a broad focus, including but not limited to:</p>

    <p>1. Metrics, benchmarks, and evaluation of trustworthy LLMs</p>
    <p>2. Improving reliability and truthfulness of LLMs</p>
    <p>3. Explainability and interpretability of language model responses</p>
    <p>4. Robustness of LLMs</p>
    <p>5. Unlearning for LLMs</p>
    <p>6. Fairness of LLMs</p>
    <p>7. Guardrails and regulations for LLMs</p>
    <p>8. Error detection and correction</p>

    

    <!--    <h2>Organizers</h2>-->

    
    
    <!-- <h2>Sponsors</h2>
    <div class="sponsors-logos">
        <a href="https://www.umiacs.umd.edu/" target="_blank">
            <img src="assets/umiacs-building-logo.png" alt="UMIACS Logo" title="UMIACS">
        </a> -->
        <!--        <a href="http://sponsor2.com" target="_blank">-->
        <!--            <img src="images/sponsors/sponsor2.png" alt="Sponsor 2 Logo" title="Sponsor 2">-->
        <!--        </a>-->
        <!-- Add more sponsors as needed -->
    <!-- </div> -->
    <h2>Organizers</h2>
    <div class="photo-grid">
        <figure class="photo">
            <img src="assets/people/micah.jpg" alt="Micah Goldblum">
            <figcaption alt="Micah Goldblum"><a href="https://goldblum.github.io/">Micah Goldblum</a><br>Columbia</figcaption>
        </figure>
        <figure class="photo">
            <img src="assets/people/rnarayanam.png" alt="Ramasuri Narayanam">
            <figcaption alt="Ramasuri Narayanam"><a href="https://research.adobe.com/person/ramasuri-narayanam/">Ramasuri Narayanam</a><br>Adobe </figcaption>
        </figure>
        <figure class="photo">
            <img src="assets/people/bang.jpg" alt="Bang An">
            <figcaption alt="Bang An"><a href="https://bangann.github.io/">Bang An</a><br>UMD </figcaption>
        </figure>
        <figure class="photo">
            <img src="assets/people/soumya.jpg" alt="Soumyabrata Pal">
            <figcaption alt="Soumyabrata Pal"><a href="https://soumyabratap.github.io/">Soumyabrata Pal</a><br>Adobe</figcaption>
        </figure>
        <figure class="photo">
            <img src="assets/people/martin.jpg" alt="Martin Pawelczyk">
            <figcaption alt="Martin Pawelczyk"><a href="https://sites.google.com/view/martinpawelczyk/">Martin Pawelczyk</a><br>Harvard </figcaption>
        </figure>
        <figure class="photo">
            <img src="assets/people/hima-2019.jpg" alt="HimaBindu Lakkaraju">
            <figcaption alt="HimaBindu Lakkaraju"><a href="https://himalakkaraju.github.io/">HimaBindu Lakkaraju</a><br>Harvard </figcaption>
        </figure>
        <figure class="photo">
            <img src="assets/people/shsaini.jpeg" alt="Shiv Kumar Saini">
            <figcaption alt="Shiv Kumar Saini"><a href="https://research.adobe.com/person/shiv-kumar-saini/">Shiv Kumar Saini</a><br>Adobe </figcaption>
        </figure>
        <!-- Add more photos as needed -->
    </div>


    <!-- <h2>Program Committee</h2>
    <div class="committee-list">
        <p>Aarshvi Gajjar, Abhimanyu Hans, Aditya Kumar, Alex Stein, Alicia Sagae, Ameya Joshi, Aniruddha Saha, Anna Sotnikova, 
        Anne Josiane Kouam, Antoni Kowalczuk, Arpit Bansal, Avi Schwarzschild, Ayesha Ansar, Ben Feuer, Bihe Zhao, Charvi Rastogi, 
        Chejian Xu, Chirag Nagpal, Chulin Xie, CJ Lee, Daniel Arp, Dariush Wahdany, David Beste, David Pape, Dongxian Wu, Emily Black, 
        Emily Wenger, Felix Weißberg, Fengqing Jiang, Feyza Duman, Gauri Jagatap, Govind Mittal, Grace Kim, Hamid Kazemi, 
        Hojjat Aghakhani, Hossein Hajipour, Hossein Souri, Jan Dubiński, Jing Xu, Joel Frank, John Kitchenbauer, Jonas Möller, 
        Jonas Ricker, Jonathan Evertz, Kelly Marshall, Kezhi Kong, Khalid Saifullah, Lea Schönher, Liu Leqi, Manli Shu, Martin Bertran Lopez, 
        Mathew Monfort, Mayuka Jayawardhana, Michael Feffer, Michel Meintz, Minh Pham, Minsu Cho, Mintong Kang, Mohammadreza Soltani, 
        Naren Dhyani, Nari Johnson, Neel Jain, Nupur Kulkarni, Olatunji Iyiola Emmanuel, Patrick Yubeaton, Pranamesh Chakraborty, 
        Pratyush Maini, Pruthuvi Maheshakya Wijewardena, Qi Zhang, Renkun Ni, Riccardo Fogliato, Roman Levin, Rongting Zhang, 
        Sahar Abdelnabi, Sahil Verma, Sai Pranaswi, Sandeep Avula, Shahrzad Kianidehkordi, Shantanu Gupta, Shawn Shan, Shreya Agarwal, 
        Sina Däubener, Srishti Gupta, Sudipta Banerjee, Thanh Nguyen, Thorsten Eisenhofer, Tianqi Du, Tom Blanchard, Vasu Singla, 
        Vedant Nanda, Vijay Keswani, Vincent Hanke, Wenhao Wang, Xun Wang, Zeming Wei, Yi Zheng, Yichuan Mo, Zhangchen Xu, Zhijian Zhuo, 
        Zuowen Yuan</p>
    </div> -->

<!-- <h2>Acknowledgements</h2>

<p>We would like to extend our heartfelt thanks to all those who made this Workshop on Trustworthy LLMs a success. This event would not have been possible without valuable contributions from several individuals.
Lastly, we express our gratitude to everyone working towards creating more trustworthy, reliable, and ethical AI systems. It is only through collaboration and shared knowledge that we can continue to push the boundaries of what is possible in AI while ensuring it remains safe and responsible for all.
Thank you all for being part of this journey!</p>


<h2>Diversity commitment</h2>
    <p>
        Commitment to diversity is especially crucial in the field of AI safety, where the fears and AI use-cases of different communities vary.  Our workshop is committed to creating a diverse and inclusive environment, as evidenced by our lists of organizers, speakers, and panelists which are diverse in terms of gender, ethnicity, geographic location, seniority, and affiliation. By bringing together a wide range of perspectives, we aim to address the multifaceted challenges in trustworthy AI more effectively. Our organizer, speaker, and panelist rosters include 5 women, researchers from and living in three continents, those at a variety of seniority levels, and many people from both academia and industry.  We will further promote these kinds of diversity when advertising our workshop and implementing accessibility features.

    </p> -->

<h2>Contact</h2>
    <p>
        To mail the organizers, please send an email to 
        <a href="mailto:iclr-workshop-building-trust@googlegroups.com">iclr-workshop-building-trust@googlegroups.com</a>, 
        and please CC the following contacts:
    </p>
    <p>
        <strong>Correspondence:</strong><br>
        Ramasuri Narayanam: 
        <a href="mailto:rnarayanam@adobe.com">rnarayanam@adobe.com</a>,<br>
        Martin Pawelczyk: 
        <a href="mailto:martin.pawelczyk.1@gmail.com">martin.pawelczyk.1@gmail.com</a>
    </p>

</div>
<div class="footer">
    <p>ICLR 2025 Workshop - Building Trust in LLMs and LLM Applications: From Guardrails to Explainability to Regulation</p>
</div>
</body>
</html>
