<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Poster Session 2 - ICLR 2025 Workshop on Trustworthy LLM</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" href="assets/image.png" type="image/x-icon">
</head>
<body>
    <div class="header">
        <h1>ICLR 2025 Workshop<br> Building Trust in LLMs and LLM Applications: <br>From Guardrails to Explainability to Regulation</h1>
    </div>
    <div class="nav">
        <a href="index.html">HOME</a>
        <a href="speakers.html">SPEAKERS AND PANELISTS</a>
        <a href="cfp.html">CALL FOR PAPERS</a>
        <a href="schedule.html">WORKSHOP SCHEDULE</a>
    </div>
    <div class="content">
        <h2>Poster Session 2 (5:00pm-6:00pm)</h2>
        <div class="poster-list">
            <div class="poster">
                <p>AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors</p>
            </div>
            <div class="poster">
                <p>Dynaseal: A Backend-Controlled LLM API Key Distribution Scheme with Constrained Invocation Parameters</p>
            </div>
            <div class="poster">
                <p>How Does Entropy Influence Modern Text-to-SQL Systems?</p>
            </div>
            <div class="poster">
                <p>Language Models Use Trigonometry to Do Addition</p>
            </div>
            <div class="poster">
                <p>Black-Box Adversarial Attacks on LLM-Based Code Completion</p>
            </div>
            <div class="poster">
                <p>Learning Automata from Demonstrations, Examples, and Natural Language</p>
            </div>
            <div class="poster">
                <p>CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models</p>
            </div>
            <div class="poster">
                <p>MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models</p>
            </div>
            <div class="poster">
                <p>Privately Learning from Graphs with Applications in Fine-tuning Large Pretrained Models</p>
            </div>
            <div class="poster">
                <p>Automated Red Teaming with GOAT: the Generative Offensive Agent Tester</p>
            </div>
            <div class="poster">
                <p>Building Bridges, Not Walls: Advancing Interpretability by Unifying Feature, Data, and Model Component Attribution</p>
            </div>
            <div class="poster">
                <p>ToolScan: A Benchmark For Characterizing Errors In Tool-Use LLMs</p>
            </div>
            <div class="poster">
                <p>Model Evaluations Need Rigorous and Transparent Human Baselines</p>
            </div>
            <div class="poster">
                <p>Automated Feature Labeling with Token-Space Gradient Descent</p>
            </div>
            <div class="poster">
                <p>Automated Capability Discovery via Model Self-Exploration</p>
            </div>
            <div class="poster">
                <p>ExpProof : Operationalizing Explanations for Confidential Models with ZKPs</p>
            </div>
            <div class="poster">
                <p>Boosting Adversarial Robustness of Vision-Language Pre-training Models against Multimodal Adversarial attacks</p>
            </div>
            <div class="poster">
                <p>Evaluation of Large Language Models via Coupled Token Generation</p>
            </div>
            <div class="poster">
                <p>Red Teaming for Trust: Evaluating Multicultural and Multilingual AI Systems in Asia-Pacific</p>
            </div>
            <div class="poster">
                <p>Has My System Prompt Been Used? Large Language Model Prompt Membership Inference</p>
            </div>
            <div class="poster">
                <p>Why Do Multiagent Systems Fail?</p>
            </div>
            <div class="poster">
                <p>Do Multilingual LLMs Think In English?</p>
            </div>
            <div class="poster">
                <p>Monitoring LLM Agents for Sequentially Contextual Harm</p>
            </div>
            <div class="poster">
                <p>BaxBench: Can LLMs Generate Correct and Secure Backends?</p>
            </div>
            <div class="poster">
                <p>Integrated Gradients Provides Faithful Language Model Attributions for In-Context Learning</p>
            </div>
            <div class="poster">
                <p>HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild</p>
            </div>
            <div class="poster">
                <p>MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered</p>
            </div>
            <div class="poster">
                <p>Disentangling Sequence Memorization and General Capability  in Large Language Models</p>
            </div>
            <div class="poster">
                <p>Unlearning Geo-Cultural Stereotypes in Multilingual LLMs</p>
            </div>
            <div class="poster">
                <p>On the Role of Prompt Multiplicity in LLM Hallucination Evaluation</p>
            </div>
            <div class="poster">
                <p>The Fundamental Limits of LLM Unlearning: Complexity-Theoretic Barriers and Provably Optimal Protocols</p>
            </div>
            <div class="poster">
                <p>An Empirical Study on Prompt Compression for Large Language Models</p>
            </div>
            <div class="poster">
                <p>Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information</p>
            </div>
            <div class="poster">
                <p>Temporally Sparse Attack for Fooling Large Language Models in Time Series Forecasting</p>
            </div>
            <div class="poster">
                <p>Maybe I Should Not Answer That, but... Do LLMs Understand The Safety of Their Inputs?</p>
            </div>
            <div class="poster">
                <p>Endive: A Cross-Dialect Benchmark for Fairness and Performance in Large Language Models</p>
            </div>
            <div class="poster">
                <p>Enhancing CBMs Through Binary Distillation with Applications to Test-Time Intervention</p>
            </div>
            <div class="poster">
                <p>Understanding (Un)Reliability of Steering Vectors in Language Models</p>
            </div>
            <div class="poster">
                <p>Towards Understanding Distilled Reasoning Models: A Representational Approach</p>
            </div>
            <div class="poster">
                <p>Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study</p>
            </div>
            <div class="poster">
                <p>Conformal Structured Prediction</p>
            </div>
            <div class="poster">
                <p>LLM Neurosurgeon: Targeted Knowledge Removal in LLMs using Sparse Autoencoders</p>
            </div>
            <div class="poster">
                <p>TEMPEST: Multi-Turn Jailbreaking of Large Language Models with Tree Search</p>
            </div>
            <div class="poster">
                <p>The Steganographic Potentials of Language Models</p>
            </div>
            <div class="poster">
                <p>Unlocking Hierarchical Concept Discovery in Language Models Through Geometric Regularization</p>
            </div>
        </div>
    </div>
    <div class="footer">
        <p>ICLR 2025 Workshop - Building Trust in LLMs and LLM Applications: From Guardrails to Explainability to Regulation</p>
    </div>
</body>
</html> 